Using 16bit native Automatic Mixed Precision (AMP)
D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
  rank_zero_warn(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\configuration_validator.py:133: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
  rank_zero_warn("You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Finding best initial lr:   0%|          | 0/10000 [00:00<?, ?it/s]D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\optim\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "


































Finding best initial lr:   2%|▏         | 168/10000 [01:10<1:11:30,  2.29it/s]Traceback (most recent call last):
  File "D:/project/kang_bart/main.py", line 73, in <module>
    main()
  File "D:/project/kang_bart/main.py", line 56, in main
    new_lr = custom_lr_finder(model, train_loader)
  File "D:\project\kang_bart\utils\lr_finder.py", line 11, in custom_lr_finder
    lr_finder = trainer.tuner.lr_find(model, train_dataloaders=train_loader, num_training=num_training)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\tuning.py", line 192, in lr_find
    result = self.trainer.tune(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1126, in tune
    result = self.tuner._tune(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\tuning.py", line 63, in _tune
    result["lr_find"] = lr_find(self.trainer, model, **lr_find_kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\lr_finder.py", line 224, in lr_find
    trainer.tuner._run(model)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\tuning.py", line 73, in _run
    self.trainer._run(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1234, in _run
    results = self._run_stage()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1321, in _run_stage
    return self._run_train()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1351, in _run_train
    self.fit_loop.run()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\fit_loop.py", line 268, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\epoch\training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1593, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\core\lightning.py", line 1644, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\core\optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\strategies\strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\plugins\precision\native_amp.py", line 85, in optimizer_step
    closure_result = closure()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 143, in closure
    self._backward_fn(step_output.closure_loss)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 311, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1763, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\strategies\strategy.py", line 168, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\plugins\precision\precision_plugin.py", line 80, in backward
    model.backward(closure_loss, optimizer, *args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\core\lightning.py", line 1389, in backward
    loss.backward(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
