{"format": "torch", "nodes": [{"name": "model", "id": 2483068248016, "class_name": "BartForConditionalGeneration(\n  (model): BartModel(\n    (shared): Embedding(32000, 768, padding_idx=0)\n    (encoder): BartEncoder(\n      (embed_tokens): Embedding(32000, 768, padding_idx=0)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): BartEncoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): Embedding(32000, 768, padding_idx=0)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): BartDecoderLayer(\n          (self_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n)", "parameters": [["model.shared.weight", [32000, 768]], ["model.encoder.embed_positions.weight", [1026, 768]], ["model.encoder.layers.0.self_attn.k_proj.weight", [768, 768]], ["model.encoder.layers.0.self_attn.k_proj.bias", [768]], ["model.encoder.layers.0.self_attn.v_proj.weight", [768, 768]], ["model.encoder.layers.0.self_attn.v_proj.bias", [768]], ["model.encoder.layers.0.self_attn.q_proj.weight", [768, 768]], ["model.encoder.layers.0.self_attn.q_proj.bias", [768]], ["model.encoder.layers.0.self_attn.out_proj.weight", [768, 768]], ["model.encoder.layers.0.self_attn.out_proj.bias", [768]], ["model.encoder.layers.0.self_attn_layer_norm.weight", [768]], ["model.encoder.layers.0.self_attn_layer_norm.bias", [768]], ["model.encoder.layers.0.fc1.weight", [3072, 768]], ["model.encoder.layers.0.fc1.bias", [3072]], ["model.encoder.layers.0.fc2.weight", [768, 3072]], ["model.encoder.layers.0.fc2.bias", [768]], ["model.encoder.layers.0.final_layer_norm.weight", [768]], ["model.encoder.layers.0.final_layer_norm.bias", [768]], ["model.encoder.layers.1.self_attn.k_proj.weight", [768, 768]], ["model.encoder.layers.1.self_attn.k_proj.bias", [768]], ["model.encoder.layers.1.self_attn.v_proj.weight", [768, 768]], ["model.encoder.layers.1.self_attn.v_proj.bias", [768]], ["model.encoder.layers.1.self_attn.q_proj.weight", [768, 768]], ["model.encoder.layers.1.self_attn.q_proj.bias", [768]], ["model.encoder.layers.1.self_attn.out_proj.weight", [768, 768]], ["model.encoder.layers.1.self_attn.out_proj.bias", [768]], ["model.encoder.layers.1.self_attn_layer_norm.weight", [768]], ["model.encoder.layers.1.self_attn_layer_norm.bias", [768]], ["model.encoder.layers.1.fc1.weight", [3072, 768]], ["model.encoder.layers.1.fc1.bias", [3072]], ["model.encoder.layers.1.fc2.weight", [768, 3072]], ["model.encoder.layers.1.fc2.bias", [768]], ["model.encoder.layers.1.final_layer_norm.weight", [768]], ["model.encoder.layers.1.final_layer_norm.bias", [768]], ["model.encoder.layers.2.self_attn.k_proj.weight", [768, 768]], ["model.encoder.layers.2.self_attn.k_proj.bias", [768]], ["model.encoder.layers.2.self_attn.v_proj.weight", [768, 768]], ["model.encoder.layers.2.self_attn.v_proj.bias", [768]], ["model.encoder.layers.2.self_attn.q_proj.weight", [768, 768]], ["model.encoder.layers.2.self_attn.q_proj.bias", [768]], ["model.encoder.layers.2.self_attn.out_proj.weight", [768, 768]], ["model.encoder.layers.2.self_attn.out_proj.bias", [768]], ["model.encoder.layers.2.self_attn_layer_norm.weight", [768]], ["model.encoder.layers.2.self_attn_layer_norm.bias", [768]], ["model.encoder.layers.2.fc1.weight", [3072, 768]], ["model.encoder.layers.2.fc1.bias", [3072]], ["model.encoder.layers.2.fc2.weight", [768, 3072]], ["model.encoder.layers.2.fc2.bias", [768]], ["model.encoder.layers.2.final_layer_norm.weight", [768]], ["model.encoder.layers.2.final_layer_norm.bias", [768]], ["model.encoder.layers.3.self_attn.k_proj.weight", [768, 768]], ["model.encoder.layers.3.self_attn.k_proj.bias", [768]], ["model.encoder.layers.3.self_attn.v_proj.weight", [768, 768]], ["model.encoder.layers.3.self_attn.v_proj.bias", [768]], ["model.encoder.layers.3.self_attn.q_proj.weight", [768, 768]], ["model.encoder.layers.3.self_attn.q_proj.bias", [768]], ["model.encoder.layers.3.self_attn.out_proj.weight", [768, 768]], ["model.encoder.layers.3.self_attn.out_proj.bias", [768]], ["model.encoder.layers.3.self_attn_layer_norm.weight", [768]], ["model.encoder.layers.3.self_attn_layer_norm.bias", [768]], ["model.encoder.layers.3.fc1.weight", [3072, 768]], ["model.encoder.layers.3.fc1.bias", [3072]], ["model.encoder.layers.3.fc2.weight", [768, 3072]], ["model.encoder.layers.3.fc2.bias", [768]], ["model.encoder.layers.3.final_layer_norm.weight", [768]], ["model.encoder.layers.3.final_layer_norm.bias", [768]], ["model.encoder.layers.4.self_attn.k_proj.weight", [768, 768]], ["model.encoder.layers.4.self_attn.k_proj.bias", [768]], ["model.encoder.layers.4.self_attn.v_proj.weight", [768, 768]], ["model.encoder.layers.4.self_attn.v_proj.bias", [768]], ["model.encoder.layers.4.self_attn.q_proj.weight", [768, 768]], ["model.encoder.layers.4.self_attn.q_proj.bias", [768]], ["model.encoder.layers.4.self_attn.out_proj.weight", [768, 768]], ["model.encoder.layers.4.self_attn.out_proj.bias", [768]], ["model.encoder.layers.4.self_attn_layer_norm.weight", [768]], ["model.encoder.layers.4.self_attn_layer_norm.bias", [768]], ["model.encoder.layers.4.fc1.weight", [3072, 768]], ["model.encoder.layers.4.fc1.bias", [3072]], ["model.encoder.layers.4.fc2.weight", [768, 3072]], ["model.encoder.layers.4.fc2.bias", [768]], ["model.encoder.layers.4.final_layer_norm.weight", [768]], ["model.encoder.layers.4.final_layer_norm.bias", [768]], ["model.encoder.layers.5.self_attn.k_proj.weight", [768, 768]], ["model.encoder.layers.5.self_attn.k_proj.bias", [768]], ["model.encoder.layers.5.self_attn.v_proj.weight", [768, 768]], ["model.encoder.layers.5.self_attn.v_proj.bias", [768]], ["model.encoder.layers.5.self_attn.q_proj.weight", [768, 768]], ["model.encoder.layers.5.self_attn.q_proj.bias", [768]], ["model.encoder.layers.5.self_attn.out_proj.weight", [768, 768]], ["model.encoder.layers.5.self_attn.out_proj.bias", [768]], ["model.encoder.layers.5.self_attn_layer_norm.weight", [768]], ["model.encoder.layers.5.self_attn_layer_norm.bias", [768]], ["model.encoder.layers.5.fc1.weight", [3072, 768]], ["model.encoder.layers.5.fc1.bias", [3072]], ["model.encoder.layers.5.fc2.weight", [768, 3072]], ["model.encoder.layers.5.fc2.bias", [768]], ["model.encoder.layers.5.final_layer_norm.weight", [768]], ["model.encoder.layers.5.final_layer_norm.bias", [768]], ["model.encoder.layernorm_embedding.weight", [768]], ["model.encoder.layernorm_embedding.bias", [768]], ["model.decoder.embed_positions.weight", [1026, 768]], ["model.decoder.layers.0.self_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.0.self_attn.k_proj.bias", [768]], ["model.decoder.layers.0.self_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.0.self_attn.v_proj.bias", [768]], ["model.decoder.layers.0.self_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.0.self_attn.q_proj.bias", [768]], ["model.decoder.layers.0.self_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.0.self_attn.out_proj.bias", [768]], ["model.decoder.layers.0.self_attn_layer_norm.weight", [768]], ["model.decoder.layers.0.self_attn_layer_norm.bias", [768]], ["model.decoder.layers.0.encoder_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.0.encoder_attn.k_proj.bias", [768]], ["model.decoder.layers.0.encoder_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.0.encoder_attn.v_proj.bias", [768]], ["model.decoder.layers.0.encoder_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.0.encoder_attn.q_proj.bias", [768]], ["model.decoder.layers.0.encoder_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.0.encoder_attn.out_proj.bias", [768]], ["model.decoder.layers.0.encoder_attn_layer_norm.weight", [768]], ["model.decoder.layers.0.encoder_attn_layer_norm.bias", [768]], ["model.decoder.layers.0.fc1.weight", [3072, 768]], ["model.decoder.layers.0.fc1.bias", [3072]], ["model.decoder.layers.0.fc2.weight", [768, 3072]], ["model.decoder.layers.0.fc2.bias", [768]], ["model.decoder.layers.0.final_layer_norm.weight", [768]], ["model.decoder.layers.0.final_layer_norm.bias", [768]], ["model.decoder.layers.1.self_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.1.self_attn.k_proj.bias", [768]], ["model.decoder.layers.1.self_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.1.self_attn.v_proj.bias", [768]], ["model.decoder.layers.1.self_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.1.self_attn.q_proj.bias", [768]], ["model.decoder.layers.1.self_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.1.self_attn.out_proj.bias", [768]], ["model.decoder.layers.1.self_attn_layer_norm.weight", [768]], ["model.decoder.layers.1.self_attn_layer_norm.bias", [768]], ["model.decoder.layers.1.encoder_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.1.encoder_attn.k_proj.bias", [768]], ["model.decoder.layers.1.encoder_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.1.encoder_attn.v_proj.bias", [768]], ["model.decoder.layers.1.encoder_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.1.encoder_attn.q_proj.bias", [768]], ["model.decoder.layers.1.encoder_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.1.encoder_attn.out_proj.bias", [768]], ["model.decoder.layers.1.encoder_attn_layer_norm.weight", [768]], ["model.decoder.layers.1.encoder_attn_layer_norm.bias", [768]], ["model.decoder.layers.1.fc1.weight", [3072, 768]], ["model.decoder.layers.1.fc1.bias", [3072]], ["model.decoder.layers.1.fc2.weight", [768, 3072]], ["model.decoder.layers.1.fc2.bias", [768]], ["model.decoder.layers.1.final_layer_norm.weight", [768]], ["model.decoder.layers.1.final_layer_norm.bias", [768]], ["model.decoder.layers.2.self_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.2.self_attn.k_proj.bias", [768]], ["model.decoder.layers.2.self_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.2.self_attn.v_proj.bias", [768]], ["model.decoder.layers.2.self_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.2.self_attn.q_proj.bias", [768]], ["model.decoder.layers.2.self_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.2.self_attn.out_proj.bias", [768]], ["model.decoder.layers.2.self_attn_layer_norm.weight", [768]], ["model.decoder.layers.2.self_attn_layer_norm.bias", [768]], ["model.decoder.layers.2.encoder_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.2.encoder_attn.k_proj.bias", [768]], ["model.decoder.layers.2.encoder_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.2.encoder_attn.v_proj.bias", [768]], ["model.decoder.layers.2.encoder_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.2.encoder_attn.q_proj.bias", [768]], ["model.decoder.layers.2.encoder_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.2.encoder_attn.out_proj.bias", [768]], ["model.decoder.layers.2.encoder_attn_layer_norm.weight", [768]], ["model.decoder.layers.2.encoder_attn_layer_norm.bias", [768]], ["model.decoder.layers.2.fc1.weight", [3072, 768]], ["model.decoder.layers.2.fc1.bias", [3072]], ["model.decoder.layers.2.fc2.weight", [768, 3072]], ["model.decoder.layers.2.fc2.bias", [768]], ["model.decoder.layers.2.final_layer_norm.weight", [768]], ["model.decoder.layers.2.final_layer_norm.bias", [768]], ["model.decoder.layers.3.self_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.3.self_attn.k_proj.bias", [768]], ["model.decoder.layers.3.self_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.3.self_attn.v_proj.bias", [768]], ["model.decoder.layers.3.self_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.3.self_attn.q_proj.bias", [768]], ["model.decoder.layers.3.self_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.3.self_attn.out_proj.bias", [768]], ["model.decoder.layers.3.self_attn_layer_norm.weight", [768]], ["model.decoder.layers.3.self_attn_layer_norm.bias", [768]], ["model.decoder.layers.3.encoder_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.3.encoder_attn.k_proj.bias", [768]], ["model.decoder.layers.3.encoder_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.3.encoder_attn.v_proj.bias", [768]], ["model.decoder.layers.3.encoder_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.3.encoder_attn.q_proj.bias", [768]], ["model.decoder.layers.3.encoder_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.3.encoder_attn.out_proj.bias", [768]], ["model.decoder.layers.3.encoder_attn_layer_norm.weight", [768]], ["model.decoder.layers.3.encoder_attn_layer_norm.bias", [768]], ["model.decoder.layers.3.fc1.weight", [3072, 768]], ["model.decoder.layers.3.fc1.bias", [3072]], ["model.decoder.layers.3.fc2.weight", [768, 3072]], ["model.decoder.layers.3.fc2.bias", [768]], ["model.decoder.layers.3.final_layer_norm.weight", [768]], ["model.decoder.layers.3.final_layer_norm.bias", [768]], ["model.decoder.layers.4.self_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.4.self_attn.k_proj.bias", [768]], ["model.decoder.layers.4.self_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.4.self_attn.v_proj.bias", [768]], ["model.decoder.layers.4.self_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.4.self_attn.q_proj.bias", [768]], ["model.decoder.layers.4.self_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.4.self_attn.out_proj.bias", [768]], ["model.decoder.layers.4.self_attn_layer_norm.weight", [768]], ["model.decoder.layers.4.self_attn_layer_norm.bias", [768]], ["model.decoder.layers.4.encoder_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.4.encoder_attn.k_proj.bias", [768]], ["model.decoder.layers.4.encoder_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.4.encoder_attn.v_proj.bias", [768]], ["model.decoder.layers.4.encoder_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.4.encoder_attn.q_proj.bias", [768]], ["model.decoder.layers.4.encoder_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.4.encoder_attn.out_proj.bias", [768]], ["model.decoder.layers.4.encoder_attn_layer_norm.weight", [768]], ["model.decoder.layers.4.encoder_attn_layer_norm.bias", [768]], ["model.decoder.layers.4.fc1.weight", [3072, 768]], ["model.decoder.layers.4.fc1.bias", [3072]], ["model.decoder.layers.4.fc2.weight", [768, 3072]], ["model.decoder.layers.4.fc2.bias", [768]], ["model.decoder.layers.4.final_layer_norm.weight", [768]], ["model.decoder.layers.4.final_layer_norm.bias", [768]], ["model.decoder.layers.5.self_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.5.self_attn.k_proj.bias", [768]], ["model.decoder.layers.5.self_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.5.self_attn.v_proj.bias", [768]], ["model.decoder.layers.5.self_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.5.self_attn.q_proj.bias", [768]], ["model.decoder.layers.5.self_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.5.self_attn.out_proj.bias", [768]], ["model.decoder.layers.5.self_attn_layer_norm.weight", [768]], ["model.decoder.layers.5.self_attn_layer_norm.bias", [768]], ["model.decoder.layers.5.encoder_attn.k_proj.weight", [768, 768]], ["model.decoder.layers.5.encoder_attn.k_proj.bias", [768]], ["model.decoder.layers.5.encoder_attn.v_proj.weight", [768, 768]], ["model.decoder.layers.5.encoder_attn.v_proj.bias", [768]], ["model.decoder.layers.5.encoder_attn.q_proj.weight", [768, 768]], ["model.decoder.layers.5.encoder_attn.q_proj.bias", [768]], ["model.decoder.layers.5.encoder_attn.out_proj.weight", [768, 768]], ["model.decoder.layers.5.encoder_attn.out_proj.bias", [768]], ["model.decoder.layers.5.encoder_attn_layer_norm.weight", [768]], ["model.decoder.layers.5.encoder_attn_layer_norm.bias", [768]], ["model.decoder.layers.5.fc1.weight", [3072, 768]], ["model.decoder.layers.5.fc1.bias", [3072]], ["model.decoder.layers.5.fc2.weight", [768, 3072]], ["model.decoder.layers.5.fc2.bias", [768]], ["model.decoder.layers.5.final_layer_norm.weight", [768]], ["model.decoder.layers.5.final_layer_norm.bias", [768]], ["model.decoder.layernorm_embedding.weight", [768]], ["model.decoder.layernorm_embedding.bias", [768]]], "output_shape": [[[[0], [0], [0], 0], [0, 0, [0], [0], [0], 0], [[0], [0], [0], 0, [0], 0, [0], [0], 0, [0], 0, 0, 0, [0], 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], "num_parameters": [24576000, 787968, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 768, 768, 787968, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 768, 768]}], "edges": []}