Using 16bit native Automatic Mixed Precision (AMP)
D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
  rank_zero_warn(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\configuration_validator.py:133: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
  rank_zero_warn("You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Finding best initial lr:   0%|          | 0/10000 [00:00<?, ?it/s]D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\optim\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "

















































Finding best initial lr:   5%|â–Œ         | 525/10000 [01:41<30:19,  5.21it/s]Traceback (most recent call last):
  File "D:/project/kang_bart/main.py", line 73, in <module>
    main()
  File "D:/project/kang_bart/main.py", line 56, in main
    new_lr = custom_lr_finder(model, train_loader)
  File "D:\project\kang_bart\utils\lr_finder.py", line 12, in custom_lr_finder
    lr_finder = trainer.tuner.lr_find(model, train_dataloaders=train_loader, num_training=num_training)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\tuning.py", line 192, in lr_find
    result = self.trainer.tune(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1126, in tune
    result = self.tuner._tune(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\tuning.py", line 63, in _tune
    result["lr_find"] = lr_find(self.trainer, model, **lr_find_kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\lr_finder.py", line 224, in lr_find
    trainer.tuner._run(model)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\tuning.py", line 73, in _run
    self.trainer._run(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1234, in _run
    results = self._run_stage()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1321, in _run_stage
    return self._run_train()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1351, in _run_train
    self.fit_loop.run()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\fit_loop.py", line 268, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\epoch\training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1593, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\core\lightning.py", line 1644, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\core\optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\strategies\strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\plugins\precision\native_amp.py", line 93, in optimizer_step
    step_output = self.scaler.step(optimizer, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\cuda\amp\grad_scaler.py", line 338, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\cuda\amp\grad_scaler.py", line 284, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\cuda\amp\grad_scaler.py", line 284, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
