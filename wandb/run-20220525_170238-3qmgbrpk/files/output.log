Using 16bit native Automatic Mixed Precision (AMP)
D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
  rank_zero_warn(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\configuration_validator.py:133: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
  rank_zero_warn("You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Finding best initial lr:   0%|          | 0/10000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "D:\conda\envs\NLP_Translator_39\lib\contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\utilities\seed.py", line 154, in isolate_rng
    yield
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1126, in tune
    result = self.tuner._tune(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\tuning.py", line 63, in _tune
    result["lr_find"] = lr_find(self.trainer, model, **lr_find_kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\lr_finder.py", line 224, in lr_find
    trainer.tuner._run(model)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\tuner\tuning.py", line 73, in _run
    self.trainer._run(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1234, in _run
    results = self._run_stage()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1321, in _run_stage
    return self._run_train()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1351, in _run_train
    self.fit_loop.run()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\fit_loop.py", line 268, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\epoch\training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1593, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\core\lightning.py", line 1644, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\core\optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\strategies\strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\plugins\precision\native_amp.py", line 85, in optimizer_step
    closure_result = closure()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\loops\optimization\optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1763, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\pytorch_lightning\strategies\strategy.py", line 333, in training_step
    return self.model.training_step(*args, **kwargs)
  File "D:\project\kang_bart\model\bart_model.py", line 34, in training_step
    train_loss, train_acc = self.__share_step(batch)
  File "D:\project\kang_bart\model\bart_model.py", line 86, in __share_step
    out = self(batch)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\project\kang_bart\model\bart_model.py", line 25, in forward
    out = self.model(input_ids=inputs['input_ids'],
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\transformers\models\bart\modeling_bart.py", line 1370, in forward
    masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\nn\modules\loss.py", line 1163, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "D:\conda\envs\NLP_Translator_39\lib\site-packages\torch\nn\functional.py", line 2996, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
